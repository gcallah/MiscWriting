# A Semiotic View of Artificial Intelligence: There's No Thirdness There

Gene Callahan


## Introduction


Various sceptical authors have addressed question of whether AI genuinely is thinking now, or will ever think. While I
think that many of their arguments are perfectly sound, they involve first getting the reader to engage in some
metaphysical thinking.

But the discipline of semiotics provides a way to demonstrate that there is a fundamental gulf between the way a human
interacts with the world and the way a machine does so. That is because humans do not interact with the world in
strictly dyadic relationships: they exhibit Peirce's "thirdness," which, as he demonstrated, cannot be decomposed into
simpler, dyadic relationships.

But computers, even when executing programs that may be called "artificial intelligence," exhibit only dyadic
relationships: their output is literally meaningless until it enters into a relationship of thirdness with a human
being. And since engaging in semiosis is at the core of intelligence, this demonstrates that there's no "there" there in AI.



## Just What Is "Artificial Intelligence"?


Artificial Intelligence (henceforth "AI") is not a technical term of computer science. Is not, for instance, comparable
to the classes of algorithms, such as divide-and-conquer, randomized, sorting, and greedy algorithm defined in the
standard textbook, *Introduction to Algorithms* (Cormen et al., 2009), which are defined by their CS theoretic
characteristics. As Bender and Hanna put it:

"To put it bluntly, "AI" is a marketing term. It doesn't refer to a coherent set of technologies. Instead, the phrase
'artificial intelligence' is deployed when the people building or selling a particular set of technologies will profit
from getting others to believe that their technology is similar to humans..." (2025, p. 5)



Playing chess was thought to be cutting edge AI... until LLMs came along.




## The Nature of Mechanical Computation


What is a computer, really?

We're going to demystify computation.


### The Universal Turing Machine


In the late 1930s, Alan Turing described a remarkably simple theoretical machine—a tape, a read-write head, and a set of
rules—that could read, write, and move along the tape. Despite its simplicity, this model showed how any mechanical
computation could be performed.

(Turing, 1937)



#### The Church–Turing Thesis


Lambda calculus

(Church, 1936a, and Church 1936b)


No hardware upgrade expands what can be computed. Only how fast.





### How Does a Digital Computer "Compute"?

Modern computers are possible because any system with two reliable states—like on/off or hot/cold—can perform computation. 
[Peirce]


We use electronics not because they're theoretically superior, but because they're far faster, even though a water or
pneumatic computer could compute the same things.





### Building a Water Computer

The purpose of the exercise here is to demystify what a computer is.

The Church-Turing thesis tells us that our water computer can compute anything that the most gargantuan AI server farm
can compute; the server farm may compute it in 10 seconds while our water computer takes 10 million years, but there is
no *theoretical* difference in what they can compute.


Water
Pipes
Electrical Wires
Hot Water
High Voltage
Cold Water
Low Voltage

Electronics

Logically Identical



### Gates

At certain points, two of our water computer's pipes meet in engineered devices called gates, which will output either
hot or cold water based upon what arrives at the gate. 



We're going to interpret hot water coming out of the gate as meaning TRUE. And we're going to say that we only want our
gate to output TRUE if both pipes leading into the gate were feeding in TRUE, in other words, hot water.


Hot


Cold + Cold
Hot + Cold
Cold + Hot
Hot + Hot


AND Gate

### Interpreting the Output


But there is nothing intrinsic to the physical set up that determines it to be an AND gate: that is our interpretation.
With the exact same physical set up, but where we choose to interpret cold water as TRUE, the device becomes the opposite gate: a NAND gate.
So, even with the simplest of logical operations that occurs inside a computer, what the result means depends upon how we want to interpret it.



We don't even have to interpret the physical set up as a logic gate.
We might instead consider it... our water park's coin flipper.
Hot is heads and cold is tails!
If we designed the right "program" at the top of the hill, we can simulate random coin flips that produce half heads and half tails.
The "program" could be a set of instructions to people with pails of hot and cold water at the top of the hill.



If we buy a bigger hill, we can set up a working, water-based computer.
It will be very slow!

Let's say we have as few as 32 open pipes at the top of the hill where we can provide our computer with input. That gives us 4 billion possible combinations of "code" and "data."

Thirty-two people at the top of the hill input a program and some data it should process by pouring a combination of hot
and cold water into the open pipes at the top of the hill.
We are at the bottom of the hill, and see that a pattern of TRUE and FALSE, or ones and zeros, or hot and cold water, is our computer's output.
What is the meaning of this output in front of us?


Unless we know the meaning (if any) that the people at the top of the hill assigned to their inputs, we cannot know what the meaning of the output is.
We just see a bunch of hot and cold water coming out of pipes.
The inputs at the top could've been intended by the people with buckets to calculate a batting average, or generate a snippet of music, or pick out a good chess move.
Whether the program is "correct" is also entirely based upon what the inputs meant to the people doing the inputting. 



The Critical Question:
What does the output mean?


Meaning assigned by humans
Inputs → interpretation → outputs
Without shared meaning: nothing



In Peircean terms, a computing machine has no thirdness.


But Don't LLMs Change All This?


Modern LLMs may seem fundamentally different from a water computer, but similar claims were made for earlier
technologies, and all overlook the Church–Turing Thesis!


In principle, any system capable of computation can perform the same calculations, 

LLM training itself begins by
stripping inputs of meaning into abstract representations.



While impressive, modern machine learning has no metaphysical implications: it searches program space for models that are "good enough" at a task. 

The key breakthrough is attention, 

So-called "hallucinations" are simply statistical results.



What LLMs Actually Do


I asked ChatGPT: "Can you generate an image for me of the head from the CS Lewis novel That Hideous Strength?"

The only connections apparently were between "head" and "hideous":



## Applying semiotics to our computer


### C.S. Peirce


Chemist, surveyor, mathematician, logician, astronomer and founder of modern semiotics and pragmatism.

Bertrand Russell: "certainly the greatest American thinker ever."

"As early as 1886, he saw that logical operations could be carried out by electrical switching circuits."






Peirce developed the concept of secondness.

When a rock tumbling down a hill meets another falling rock and deflects its course, we have a purely dyadic relationship: neither rock means anything further when hitting its companion.



Secondness







Peirce pointed out that human language, on the other hand, is intrinsically a triadic relationship.

If I shout to you "A mind-reductionist is creeping up behind you!" I am not merely sending some sound waves to your eardrums: I am also referring to a third thing: the reductionist.
Sign
"Dog"
Object
Interpretant



Thirdness





Think about our water computer: the water flowing down the hill, means nothing by itself. It is only a shared meaning between the inputters at the top of the hill and the readers at the bottom of the hill that gives the program being run what meaning it possesses.

The bits that represent your department's accounts for the year could just as well be bits that represent a musical composition or an image to display: inside the computer they only exhibit secondness: e.g., high and low voltages hitting a gate making it emit a low volatage.




Thirdness and AI






Employing thirdness in this context, sheds light upon the controversy over John Searle's "Chinese Room" argument
(Searle, 2009).

Many AI enthusiasts have tried to answer Searle's demonstration that simply mechanically producing the correct answers
for some problem does not imply that any understanding exists in that situation with what has come to be called the
"Systems Reply." As Ned Block describes it:

"The positive component goes further, saying that the whole system-man + program + board + paper + input and output
doors-does understand Chinese, even though the man who is acting as the CPU does not. If you open up your own computer,
looking for the CPU, you will find that it is just one of the many chips and other components on the main circuit-board.
The Systems Reply reminds us that the CPUs of the thinking computers we hope to have someday, will not themselves think
-- rather, they, will be parts of thinking systems." (Block 2003, p. 72)

A minor quibble here is that Block is a little confused about what he means by "CPU." He imagines us finding the CPU
chip inside our computer, indicating that that's what he is talking about. But if that were true, the entire system
could be incorporated on the CPU chip, much like Searle's maneuver internalizing the Chinese room, and so it would have
to be the CPU that is thinking. But more charitably interpreting what he is saying, he is really thinking of the logical
function of the CPU which is running a single instruction, and then reading the instruction that follows and executing
it, ad infinitum. Then we see that his reference to the chip called the "CPU" is just a minor gaffe.

Much more importantly, if we are to concede that the Chinese room "as a whole" understands Chinese, then let us consider
a a simple animal trap that closes on any animal that enters it. Does the trap understand what animals are, and know how
to trap them? After all, it *acts* just as if it understands and knows these things.

Block and the other Systems Reply advocates are heading in the right direction with their response to Searle: they
simply have not enlarged "the system" enough. The "animal trap system" is the trap, the human who devised it, and the
hunter who is using it. And it is certainly true that this system as a whole understands what animals are and knows how
to trap them.

Similarly, the Systems Reply contingent is correct that the system as a whole understands Chinese in Searle's thought
experiment. But the actual "system as a whole," is the designers of the system, the Chinese room itself, and the Chinese
speakers who are "speaking" to the room and receiving the replies that emerge from it.

What we were looking at are instances of extended cognition, where humans have projected their own thinking out into the
material world, and made some part of the world behave in a way that conforms to their thinking. Let us return briefly
to the animal trap: if it is used by a hunter to catch game, it "understands" what game is "knows" better how to catch
it. But if an enthusiast of some form of lawn bowling takes the same trap and uses it as the target into which one must
direct one's ball, then what was a trap is now a referee that decides when a point has been scored. It is the humans
employing the physical device who are deciding what its operations mean. We could say, "That device there's a way of
recording scores in a form of lawn bowling, but it accidentally caught an animal," only based upon what the person who
devised and set it out meant for it to be. By itself, the trap doesn't *mean* anything at all.


Umberto Eco, at the begining of his work, *The Limits of Interpretation* (1990, p. 1), relates a story he encountered in a 1641 book
by John Wilkins, about projecting human semiosis onto an inanimate artefact used in a semiotic episode. Wilkins is
discussing the amazement that non-literary peoples often had on encountering people who could "converse with Books."
In his story, a master sends his slave (brand new to the very idea of literacy) with a basket of figs,
and a note saying something like "Please enjoy these fifty figs!" to an acquaintance. En route, the slave eats a good
number of the figs. When he delivers them to the intended recipient, he is told that the note convicts him of having
consumed many of the figs along his way, and he faces some punishment.

Some days later, he is again given the same mission. Once again, he really wants to eat
figs. But this time, wiser from his previous experience, he hides the note under a rock, and then sits down to eat his
fill of the fruits. When he is once again called out, he comes to regard the paper as some sort of, well, "artificial
intelligence," that can spy and report on him, even from under a rock.






Philosophical Reinforcement


Hilary Putnam showed essentially the complementary result: multiple physical states can have the same meaning: the
brains of a human, an octopus, and a Martian can all "represent" the experience of pain.

People speaking different languages can mean the same thing, even though their brains must be in different states (to produce different sounds, at least).

Thus, "meaning" cannot be reduced to a physical state.



Multiple Realizability


Types of Signs


Computer programs are legisigns.




We don't know how humans do

Nothing so far suggests more than secondness

The head in C.S. Lewis: possessed by a demon!


## Intelligence without Semantics?


Given the inherent lack of semantics, and a computational device, it is understandable that some authors,
given their metaphysical presuppositions (more on this below), have attempted to
establish "cognition" without any basis in semantics. A notable effort in that vein is David Chalmers'
*On Implementing a Computation*.

After setting out his theory that maps physical states to computations through state transitions,
Chalmers writes "it will be noted that nothing in my account of computation and
implementation invokes any semantic considerations, such as the representational content of internal states... If we build
semantic considerations into the conditions for implementation, any role that computation can play in providing a
foundation for AI and cognitive science will be endangered..." (1995, p. 399)

* A footnote here about how Chalmers' view of physical states implementing cognition leads him towards panpsychism.

*** Goes Somewhere ***

Even if we accept Chalmers' view that any physical state that exhibits a certain structure is implementing an
algorithm* with the same structure, which is equivalent to thinking that algorithm, we still have an embarrassment
present for any attempt to reduce cognition to physical states: the master concept here is the *algorithm*, which is
quite distinctly a non-physical "thingie," and the physical states have the distinctly secondary role of exhibiting the
algorithm's structure. Without the *idea* of the algorithm in our minds, there would be nothing to suggest any
significant relationship between our water computer performing addition, and an electronic computer performing addition,
and a brain performing addition. 

In this, I think, is what explains Chalmers interest in panpsychism: if he wants to consistently follow through on his principle
 that any physical state that has the structure of an algorithm is in fact, running that algorithm, and
also thinking it, then we should accept that thermostats really are thinking about the temperature of our house.
[REF]

* Footnote: Chalmers uses "state machine" but that is just a particular way of picturing an algorithm.


As I hope I have shown in this paper, Chalmers is certainly correct that including semantic considerations will
"endanger" using computation as a foundation for AI and cognitive science. In fact, Chalmers' fears were over
optimistic: once we see the centrality of meaning for any semiotic process, it becomes clear that computation cannot be
the basis for anything that might reasonably be called "cognitive science," and that the correct name for what has been
going by "artificial intelligence" is "advanced automation."

In what appears to be merely wishful thinking, Chalmers cites Haugeland to claim "if you take care of the syntax, the
semantics will take care of itself" (1995, 399). Imagine trying to teach a group of students to speak, say, Hungarian,
by just teaching them the syntax of Hungarian, and then hoping that all the meaning will "take care of itself"!
And he claims that "computer designers... are not concerned with the semantic content" (1995, 399) of their work, a claim
that as a software designer I can confidently assert is not true.

Why would anyone dismiss the importance of something so essential as meaning to what we do when we
think? Although I do not wish to speculate about any particular individual, I suggest that in general, this is because
AI enthusiasts have a prior commitment to a metaphysical view, namely, some sort of "naturalistic" reductionism. Speaking
of one of the pioneering proponents of such a view in mathematics, David Hilbert, Larson writes, "[Hilbert's] larger
dream, thinly disguised, was really a worldview, a picture of the universe, as itself a mechanism. AI began taking shape
as...  a philosophical position..." (2021, pp. 14-15).


"To implement a computation is just to have a set of components that interact causally according to a certain pattern"
(1995, p. 401).

Of course, this definition renders the entire universe a giant computational device -- see Wolfram.


## Could AIs Become Conscious?

This portion of this paper is purely speculative, but I think it is worth sending out my thoughts here. In particular, I
am not denying the possibility that an AI could become conscious. What I am arguing instead is that the fact that it has
implemented a computation, in Chalmers sense, gives us no evidence at all to suggest that it might be conscious, since
it gives us no evidence that there was any meaning involved in the process.

However, since we really have no idea how we have a sense of consciousness, floating around with our biological bodies,
it would be very foolish to claim that AI consciousness is impossible. But if it ever happens, it will not be because
someone wrote an even more clever program to run on an even larger data center full of computers.

Furthermore, if an AI becomes intelligent, it will no longer be artificial — and no longer just a machine.



## Conclusion



Even if it turns out, contrary to anything we have found so far, that the physical processes in our brain can be mapped
neatly into a program that could run on the touring machine, this would still leave out the matter of meaning. Even if,
as I type this paper, my brain was running a computer program, it is, nevertheless, the case that I mean something by
what I have written here. And as we have seen, and as is admitted by even enthusiast for reducing human thought,
computation, the physical computation itself has no inherent meaning.


## Bibliography


- Bender, Emily M., and Alex Hanna (2025). *The AI Con: How to Fight Big Tech's Hype and Create the Future We Want*. First edition.
New York, NY: Harper.

- Block, Ned (2003). "Searle's Argument Against Cognitive Science,"
in *Views into the Chinese Room: New Essays on Searle and Artificial Intelligence*, ed. Preston J. Bishop, 70–79. Oxford: Oxford University Press.

- Chalmers, David J. (1995). "On Implementing a Computation." *Minds and Machines*, vol. 4, no. 4, 1994,
pp. 391–402, https://doi.org/10.1007/BF00974166.

- Chalmers, David J. (1996). *The Conscious Mind: In Search of a Fundamental Theory*. New York: Oxford University Press.

- Church, Alonzo. (1936a). "A Note on the Entscheidungsproblem." *Journal of Symbolic Logic*, 1(1), 40–41, 101–102.

- Church, Alonzo. (1936b). "An Unsolvable Problem in Elementary Number Theory." *American Journal of Mathematics*, 58, 345–363.

- Cormen, Thomas H., Charles Eric Leiserson, Ronald L. Rivest, and Clifford Stein (2009). *Introduction to Algorithms*.
Third edition. Cambridge, Mass.: MIT Press.

- Eco, Umberto (1990). *The Limits of Interpretation*. Bloomington and Indianapolis, Indiana: Indiana University Press.

- Larson, Erik J. (2021). *The Myth of Artificial Intelligence: Why Computers Can't Think the Way We Do*.
Cambridge, Massachusetts: The Belknap Press of Harvard University Press.

- Searle, John (2009). "Chinese Room Argument." *Scholarpedia Journal*, vol. 4, no. 8, p. 3100, https://doi.org/10.4249/scholarpedia.3100.

- Turing, A. M. (1937). "On Computable Numbers, with an Application to the Entscheidungsproblem".
*Proceedings of the London Mathematical Society*. Series 2, Volume 42, Issue 1, pp. 230–265. 

- Turing, A. M. (1950). "Computing Machinery and Intelligence." *Mind*, vol. 59, no. 236, pp. 433–60.

